{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00c0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Added OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer # For combining different transformers\n",
    "from sklearn.pipeline import Pipeline # For chaining steps\n",
    "import numpy as np # For numerical operations\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Define the path to your processed Parquet data on your local machine\n",
    "# This should be the folder containing the 'part-' files from your Spark processing.\n",
    "processed_data_path = \"/Users/mac/Desktop/processed_stock_data_parquet_local/\" \n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    df = pd.read_parquet(processed_data_path)\n",
    "    print(\"DataFrame loaded successfully!\")\n",
    "    print(f\"Shape of the DataFrame: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows of the DataFrame:\")\n",
    "    print(df.head()) # Display initial rows to confirm load\n",
    "    print(\"\\nDataFrame Info (data types and non-null counts):\")\n",
    "    df.info() # Display data types and non-null counts\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet data: {e}\")\n",
    "    print(\"Please ensure the 'processed_data_path' is correct and pyarrow is installed.\")\n",
    "    # It's good practice to exit or raise in notebooks if a critical step fails\n",
    "    raise # Re-raise the exception to stop execution if data load fails\n",
    "\n",
    "# --- Data Preparation for Machine Learning ---\n",
    "\n",
    "# 1. Engineer Target Variable: Predict if the price will go up (1) or down/stay same (0) tomorrow\n",
    "# Ensure data is sorted by Ticker and Date for correct 'next day' calculation\n",
    "df = df.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Calculate next day's close price for each ticker using a window function\n",
    "# .shift(-1) gets the value from the next row within each ticker group\n",
    "window_spec_next_day = df.groupby('Ticker')['Close']\n",
    "df['Next_Day_Close'] = window_spec_next_day.shift(-1)\n",
    "\n",
    "# Define the target variable: 1 if Next_Day_Close > Close (price goes up), 0 otherwise\n",
    "# We cast to int for binary classification\n",
    "df['Price_Change_Direction'] = (df['Next_Day_Close'] > df['Close']).astype(int)\n",
    "\n",
    "# Handle rows where 'Next_Day_Close' is NaN (the last day for each ticker in the dataset)\n",
    "# Also drop any rows where features engineered in Task 3 might be NaN (e.g., first few Daily_Return/SMAs)\n",
    "df = df.dropna(subset=['Next_Day_Close', 'Daily_Return', 'SMA_5_Day', 'SMA_20_Day']) \n",
    "\n",
    "print(\"\\n--- DataFrame after Target Variable Engineering ---\")\n",
    "# Display relevant columns to confirm the new target variable\n",
    "print(df[['Date', 'Ticker', 'Close', 'Next_Day_Close', 'Price_Change_Direction']].head())\n",
    "print(f\"Shape after dropping NaNs: {df.shape}\")\n",
    "print(f\"Target variable value counts (0: Down/No Change, 1: Up):\\n{df['Price_Change_Direction'].value_counts()}\")\n",
    "\n",
    "\n",
    "# 2. Define Features (X) and Target (y)\n",
    "y = df['Price_Change_Direction']\n",
    "\n",
    "# Features to use for prediction:\n",
    "# Numerical features from initial data and Task 3 processing\n",
    "numerical_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Daily_Return', 'SMA_5_Day', 'SMA_20_Day']\n",
    "\n",
    "# Temporal features extracted from 'Date'\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek # Monday=0, Sunday=6\n",
    "\n",
    "temporal_features = ['Year', 'Month', 'Day', 'DayOfWeek']\n",
    "\n",
    "# Categorical features (Ticker needs One-Hot Encoding)\n",
    "categorical_features = ['Ticker']\n",
    "\n",
    "# Combine all feature names for selection\n",
    "all_features_to_use = numerical_features + temporal_features + categorical_features\n",
    "\n",
    "X = df[all_features_to_use]\n",
    "\n",
    "print(f\"\\nShape of features (X) before preprocessing: {X.shape}\")\n",
    "print(f\"Shape of target (y): {y.shape}\")\n",
    "\n",
    "# 3. Preprocessing Pipeline for Features\n",
    "# We use ColumnTransformer to apply different transformations to different columns:\n",
    "# - 'passthrough' for numerical/temporal features (no change needed after initial processing)\n",
    "# - OneHotEncoder for the 'Ticker' categorical feature\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_temp', 'passthrough', numerical_features + temporal_features), \n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # handle_unknown='ignore' prevents errors for unseen categories\n",
    "    ])\n",
    "\n",
    "# 4. Split Data into Training and Testing Sets\n",
    "# - test_size=0.2: 20% of the data will be used for testing, 80% for training.\n",
    "# - random_state=42: Ensures reproducibility of the split.\n",
    "# - stratify=y: Crucial for classification! It ensures the proportion of target classes (Up/Down)\n",
    "#   is maintained in both training and testing sets, even if the dataset is imbalanced.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\n--- Shapes of Training and Testing Sets ---\")\n",
    "print(f\"Training features shape (before preprocessing): {X_train.shape}\")\n",
    "print(f\"Testing features shape (before preprocessing): {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "# --- Choose and Train an ML Model ---\n",
    "\n",
    "# Create a Pipeline that first applies the preprocessing (OneHotEncoding etc.)\n",
    "# and then trains the RandomForestClassifier. This ensures consistent preprocessing.\n",
    "print(\"\\n--- Initializing and Training RandomForestClassifier ---\")\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                 ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))])\n",
    "\n",
    "# Train the model using the training data (X_train, y_train)\n",
    "print(\"Training the model pipeline...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "\n",
    "# --- Evaluate Model Performance ---\n",
    "\n",
    "# Make Predictions on the Test Set (X_test)\n",
    "print(\"\\n--- Evaluating Model Performance ---\")\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Calculate Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# Use zero_division=0 to prevent warnings/errors if a class has no predicted samples\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report: Provides precision, recall, f1-score for each class\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "# Define labels for the report (0: Down/No Change, 1: Up)\n",
    "target_names = ['Down/No Change', 'Up']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))\n",
    "\n",
    "\n",
    "# --- Visualize Model Performance (Confusion Matrix) ---\n",
    "\n",
    "# Compute the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the Confusion Matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix for Stock Price Direction Prediction')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# --- Interpretation (Placeholder for your documentation) ---\n",
    "print(\"\\n--- Interpretation of Results ---\")\n",
    "print(\"These metrics indicate the model's ability to predict whether a stock's closing price will increase or decrease the next day.\")\n",
    "print(\"The Confusion Matrix visually breaks down correct vs. incorrect predictions for 'Up' and 'Down/No Change' movements.\")\n",
    "print(\"This output, along with the plot, should be included in your Task 4 documentation with a detailed interpretation in the business context, discussing how accurate predictions of stock movement can inform investment strategies or trading decisions.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
